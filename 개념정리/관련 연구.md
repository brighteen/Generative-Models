## 2장의 기존 연구 상세 설명

### 1. 볼츠만 머신 계열 (RBM, DBM)

이 모델들은 $p(x) \propto \exp(-E(x))$ 형태의 **에너지 기반 모델(Energy-Based Models)**입니다. 즉, 데이터의 확률을 "에너지"(안정적인 상태일수록 에너지가 낮음)를 통해 정의합니다.
RBM에 대해 잘 이해하기 위해서 MLE, Gradient descent, 기각 샘플링, MCMC를 알고 있어야 함.

#### 📈 RBM (Restricted Boltzmann Machines)
* **무엇인가?**: '보이는(visible)' 층(데이터 $\mathbf{x}$)과 '숨겨진(hidden)' 층($\mathbf{h}$)으로 구성된 2층짜리 **무방향성** 신경망입니다. RBM은 $p(\mathbf{x}, \mathbf{h})$라는 결합 확률 분포를 학습합니다.
* **논문의 비판 (한계점)**: $p(\mathbf{x})$를 직접 계산하려면 모든 $\mathbf{h}$에 대해 합을 구해야($p(\mathbf{x}) = \sum_{\mathbf{h}} p(\mathbf{x}, \mathbf{h})$) 하는데, 이보다 더 근본적인 문제는 확률을 정규화(총합이 1이 되게)하기 위한 **"분배 함수(Partition Function)" $Z$**를 계산해야 한다는 것입니다. $Z$는 **모든 가능한 $\mathbf{x}$**에 대해 $\exp(-E(\mathbf{x}))$를 더해야 하는데, 이는 이미지 같은 고차원 데이터에서는 **계산이 불가능(intractable)**합니다.

#### 📈 DBM (Deep Boltzmann Machines)
* **무엇인가?**: RBM을 여러 층으로 깊게 쌓은 모델입니다.
* **논문의 비판 (한계점)**: RBM보다 더 강력하지만, **$Z$의 계산 불가능성** 문제는 동일하게, 혹은 더 심각하게 겪습니다. 또한 층이 깊어지면서 학습(추론)이 훨씬 더 복잡해집니다.

#### ⛓️ MCMC (Markov Chain Monte Carlo)
* **무엇인가?**: RBM/DBM과 같은 모델에서 확률 분포를 직접 계산하는 대신, 샘플을 근사적으로 뽑기 위한 알고리즘입니다.
* **연결점**: $Z$가 계산 불가능하므로, 모델의 학습(그래디언트 계산)과 샘플 생성 모두 MCMC에 의존해야 합니다.
* **논문의 비판 (한계점)**: MCMC는 (1) 계산 비용이 매우 높고 느립니다. (2) **"믹싱(Mixing)"** 문제가 심각합니다. MCMC가 전체 분포를 골고루 탐색하지 못하고 특정 모드(mode)에 갇히게 되면(믹싱이 잘 안되면), 학습이 제대로 이루어지지 않거나 엉뚱한 샘플을 생성합니다.

---

### 2. 하이브리드 모델 (DBN)

#### 📉 DBN (Deep Belief Networks)
* **무엇인가?**: 여러 개의 RBM(무방향성)을 차례대로 쌓고 , 맨 위에는 방향성 모델을 둔 **하이브리드** 모델입니다. 딥러닝 초기에 층별 예비학습(pre-training)으로 큰 성공을 거두었습니다.
* **논문의 비판 (한계점)**: RBM과 방향성 모델을 결합했기 때문에, MCMC를 사용해야 하는 RBM의 계산적 어려움 등 **양쪽 모델의 계산적 어려움을 모두** 물려받습니다.

---

### 3. 대체 학습 기준 (Score Matching, NCE)

MCMC의 한계를 극복하기 위해, $Z$를 계산하지 않고도 모델을 학습시키려는 시도들입니다.

#### 🎯 Score Matching
* **무엇인가?**: $p(x)$의 확률 값 대신, $p(x)$의 **로그(log) 그래디언트**(소위 "score")를 실제 데이터의 score와 일치하도록 학습하는 방식입니다.
* **논문의 비판 (한계점)**: $Z$는 필요 없지만, $p(x)$의 **수식(정규화되지 않은)을 해석적으로(analytically) 알아야** 합니다. 하지만 DBM처럼 깊은 모델들은 이 수식 자체를 유도하는 것이 매우 어렵거나 불가능합니다.

#### 🧠 NCE (Noise-Contrastive Estimation)
* **무엇인가?**: **GAN과 가장 유사한 아이디어**입니다. 생성 모델 학습을 "진짜 데이터"와 "가짜 노이즈"를 구별하는 **판별(discrimination)** 문제로 바꿉니다.
* **논문의 비판 (한계점)**:
    1.  **별도의 D가 없음**: NCE는 **생성 모델(G) 자체가** 판별자(D) 역할을 겸임합니다.
    2.  **"고정된" 노이즈**: NCE의 가장 큰 한계는 "적수"가 되는 노이즈 분포가 **고정(fixed)**되어 있다는 것입니다. G가 이 쉬운 노이즈를 구별하는 법을 배우고 나면, D가 G와 함께 성장하는 GAN과 달리, NCE는 학습 신호가 약해져 **학습이 급격히 느려집니다**.

---

### 4. 암시적 모델 (GSN, VAE)

$p(x)$의 수식을 명시적으로 정의하지 않고, 샘플을 생성하는 '기계' 자체를 훈련시키는, GAN과 같은 "암시적" 모델 계열입니다.

#### 🔄 GSN (Generative Stochastic Networks)
* **무엇인가?**: 노이즈 제거 오토인코더(Denoising Auto-encoder)를 확장한 모델로, **마코프 체인의 전이(transition)** 과정을 학습합니다.
* **논문의 비판 (한계점)**:
    1.  **MCMC 필요**: GSN에서 샘플을 생성하려면, RBM처럼 학습된 **마코프 체인을 반복적으로 실행**(샘플을 네트워크에 계속 다시 넣는 피드백 루프)해야 합니다. 이는 GAN이 $G(z)$ 한 번에 샘플을 뽑는 것보다 훨씬 느리고 복잡합니다.
    2.  **ReLU 사용 어려움**: 이 **피드백 루프** 구조 때문에, 값이 폭주(unbounded)할 수 있는 **ReLU(조각별 선형 유닛)**의 이점을 활용하기 어렵습니다. 반면 GAN의 G와 D는 단순한 feed-forward 구조라 ReLU를 마음껏 사용할 수 있습니다.

#### 🧬 VAE (Auto-Encoding Variational Bayes)
* **무엇인가?**: GAN과 같은 시기(2014년 초)에 등장한 강력한 생성 모델입니다. 인코더(추론 네트워크)와 디코더(생성 네트워크)를 동시에 학습시킵니다.
* **논문의 비판 (한계점)**: VAE는 $p(x)$의 하한(ELBO)을 최대화하며, 이 과정에서 **"근사 추론(approximate inference)"**이 필요합니다. GAN은 학습이나 샘플링 과정에서 이러한 복잡한 추론 네트워크가 전혀 필요 없다 는 점에서 GAN의 단순성을 강조합니다.

---

### 요약: GAN의 혁신

GAN은 이 모든 문제들을 **우회(sidestep)** 했습니다.
* **No $Z$**: $Z$ 같은 다루기 힘든 확률 계산이 없습니다.
* **No MCMC**: 느리고 불안정한 MCMC 샘플링이 필요 없습니다.
* **Adaptive Adversary**: NCE와 달리, D가 G와 함께 성장하므로 학습이 멈추지 않습니다.
* **Fast Sampling**: GSN과 달리, $G(z)$라는 단 한 번의 forward pass로 샘플 생성이 끝납니다.
* **ReLU Friendly**: 피드백 루프가 없어 ReLU를 자유롭게 활용할 수 있습니다.