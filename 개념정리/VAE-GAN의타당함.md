고차원 데이터 공간 X에는 여러 도메인의 이미지 벡터들이 존재하지만 진정 관심있는 전체집합은 해당 고차원 공간 내 아주 얇은 부분집합에 국한됨. 그렇다면 X보다 훨씬 dense한 공간으로 매핑하는 것이 바람직함.

1. 고차원 data의 복원
그럼 일단 다루기 쉬운 저차원 공간에서 고차원 데이터의 정보가 손실되면 안됨. reconstruction(1, 재복원 오차)이 잘 되었냐가 잠재 벡터 공간 Z와 X를 연결지어줌. 이때 기준은 L1, L2 distance가 될 수 있음.

2. 잠재벡터공간
이때 잠재벡터공간이 다루기 쉬워야 하는데 이때 다루기 쉽다는 의미는 dense함과 continuous함을 의미. 그럼 밀도있고 연속적인 공간을 만들기 위해서 정규화 손실(2, kullback-leibler divergence)이 필요.

3. 잠재벡터공간에서의 탐색
- locally euclidean: 잠재벡터 공간에서의 벡터 간 거리가 유클리드 거리로 측정 가능해야 함. 즉, 벡터 간 거리가 의미가 있어야 함.
- z_i 가 population에 속해있어야 함. -> population에 속함을 discriminator(3)가 판단.
- 임의의 잠재 벡터 z_A, z_A의 성질을 보존해 끌고다닐 수 있어야 함.

이때 discriminator는 고차원 데이터 공간(X)에서 데이터를 다루는게 아닌 비교적 저차원 잠재벡터공간(Z)에서 데이터를 다루는게 논리적으로 합당하다는 것임. 그렇다면 Z 공간 내 진짜 데이터 z_real과 가짜 데이터 z_fake를 구분해야 함. 이때 z_fake의 boundary를 어떻게 정의할지 생각해봐야 함.
이 논리는 고차원 공간에서 판별자 D가 학습하는 공간이 터무니없이 광활하기에 비교적 밀도있고 저차원인 잠재벡터공간에서 학습하는게 더 좋을것임을 가정함. 최적 상태의 D*를 가정했을때 D점수를 잠재공간에 찍어보면 z_real에 대해서는 0.5, z_fake에 가까워질수록 0에 가까운 값을 가질 것임.

종국에 하려는건 잠재벡터공간에서 헷갈림의 density 지도를 만드는 것. 잠재벡터공간에서 학습데이터를 제외한 z_real은 모델이 본 적 없음. 그때 판별자를 통해 안다고 가정.
어떤 t지점의 잠재벡터 z_t가 z_B까지 이동할 때 optimal한 판별자 D*를 찾은 경우 z_t+1은 다음과 같이 정의할 수 있음.
$$z_{t+1} = z_t + \alpha(∇_z D^*(z_t) + \beta∇_z||z_B-z_t||_2^2)$$
이때 이 공간은 미분가능해야 함.

일단 인코더($q_{\phi}(z|x)$)를 학습시켜 고차원 데이터의 mu, sigma를 구함. 이때 추정한 mu를 그대로 디코더에 넣으면 재복원 잘 할거니까 이런 mu집합을 잠재벡터공간 내에서 z_real로 정의. 그럼 나머지는? 어느 영역까지 z_fake로 정의할 것인가? 

```
이때 z_fake의 경계설정이 중요한데 이 경계를 설정하는 방법은 여러가지가 있을 수 있음.
1) 잠재벡터공간 내 전체 공간을 z_fake로 정의. (단점: 너무 광활해서 판별자가 학습을 못함)
2) 잠재벡터공간 내 mu의 주변부 일정 반경 이내   를 z_fake로 정의. (단점: 반경 설정이 애매함)
3) 잠재벡터공간 내 mu의 주변부 밀도기반으로 z_fake로 정의. (장점: 밀도기반으로 경계설정이 논리적임)
```

인코더와 디코더를 학습시킨 후 z_real집합과 z_fake집합을 정의한 후 판별자 D를 학습시킴.(이때 joint로 E, G, D를 학습시킬지 따로 학습시킬지는 실험 필요, 학습과정에서 사용되는 D는 고차원 데이터를 다루는 D(x)임. 실제 탐색알고리즘에 사용할 D는 잠재벡터공간 Z내 z를 다루는 D(z)임을 유의)
이 바운더리 잡을때 empirical(경험적)하게 생각해야 함.

dense한 공간에서의 score를 찾아야 함. 이 이전까지 잠재벡터공간에서 국소적으로만 이동했는데 score를 안다는 것은 판별자가 의미하는건 헷갈림의 확률밀도인데 이 확률밀도(log)로 잠재벡터공간에서 더이상 국소적으로 이동하지 않고 멀리 이동할 수 있다는 것임.

energy based model을 생각해보면 어떤분포P(x)에 대해 그 분포의 확률밀도함수 p(x)를 직접적으로 구하지 않고 에너지함수 E(x)를 정의하여 p(x) = exp(-E(x)) / Z로 표현함. 이때 Z는 정규화 상수임. 이떄 Z를 구하는게 어려우므로 p(x)를 직접적으로 구하는게 어려움. 그래서 에너지함수를 최소화하는 방향으로 데이터를 생성하거나 샘플링하는데 현재 문제에 빗대어보면 Z가 잠재벡터공간에서 z_real, z_fake의 경계설정과 유사함. 이 문제를 정면으로 해결하기보다는 논리를 우회하는 방법을 택하는게 좋음.

---

VAE-GAN에서 판별자 D는 어떤 데이터가 진짜데이터일 확률을 출력하는데 이 확률을 에너지함수로 생각할 수 있음. 즉, D(x)가 높을수록 E(x)는 낮아지고 p(x)는 높아짐. 따라서 판별자 D를 통해 데이터의 확률밀도함수를 간접적으로 추정할 수 있음.

고차원 데이터 공간 $X$에는 다양한 도메인의 이미지 벡터가 존재하지만, 진정으로 관심 있는 데이터 집합은 해당 고차원 공간 내의 매우 얇은 매니폴드(manifold), 즉 부분집합에 국한됨. 따라서 원본 공간 $X$보다 훨씬 밀도 있고(dense) 다루기 쉬운 저차원 잠재 공간 $Z$로 데이터를 매핑하는 것이 바람직함.

이 연구는 VAE-GAN 아키텍처를 기반으로 하되, **두 개의 판별자 $D_x$와 $D_z$**를 도입하여 이 목표를 달성함.

**1. 재구성 품질 향상 ($D_x$ : Image-space Discriminator)**

잠재 공간 $Z$가 $X$의 정보를 유의미하게 보존하기 위해서는, $Z$에서 다시 $X$로의 **재구성(Reconstruction)**이 잘 이루어져야 함.

* **역할:** 전통적인 VAE의 $L_1$ 또는 $L_2$ 재구성 손실은 픽셀 값의 평균을 학습하여 **흐릿한(blurry) 이미지**를 생성하는 경향이 있음.
* **$D_x$의 도입:** 이 문제를 해결하기 위해 **픽셀 공간 판별자 $D_x$**를 사용함.
* **학습:** $D_x$는 실제 원본 이미지 $x$와, 디코더 $G$가 잠재 벡터 $z$로부터 생성한 이미지 $\hat{x} = G(z)$를 구별하도록 학습됨.
* **결과:** 디코더 $G$(또는 생성자)는 $D_x$를 속이기 위해, 픽셀 단위로 '안전한' 평균값이 아닌, $D_x$가 '진짜'라고 판단할 만큼 선명하고 사실적인 $\hat{x}$를 생성하도록 강제됨. 이는 인코더 $E$와 디코더 $G$의 **재구성 품질을 극대화**함.

**2. 잠재 공간 정규화 ($L_{KL}$ : VAE Regularization)**

잠재 공간 $Z$는 단순히 정보를 압축하는 것을 넘어 **'다루기 쉬운' 구조**를 가져야 함. 이는 $Z$가 **밀도 있고(dense) 연속적(continuous)**이어야 함을 의미함.

* **역할:** VAE의 **정규화 손실(Kullback-Leibler Divergence)**이 이 역할을 수행함.
* **수식:**
    $$L_{KL} = D_{KL}(q_{\phi}(z|x) \parallel p(z))$$
* **결과:** 이 손실은 인코더 $E$가 $x$로부터 추론한 잠재 분포 $q_{\phi}(z|x)$의 총합(aggregated posterior, $q(z)$)이, 우리가 가정한 단순한 사전 분포 $p(z)$(일반적으로 표준 정규 분포 $\mathcal{N}(0, I)$)에 근사하도록 강제함. 이는 잠재 공간을 원점 근처로 모으고 연속성을 부여함.

**3. 잠재 공간 탐색 ($D_z$ : Latent-space Discriminator)**

이 연구의 핵심 아이디어는 **잠재 공간 $Z$ 자체의 밀도를 학습**하여 탐색의 '점수(score)'로 활용하는 것임. 이를 위해 $D_x$와 별개인 **두 번째 판별자 $D_z$**를 도입함.

* **필요성:** 고차원 픽셀 공간 $X$는 판별자가 학습하기에 "터무니없이 광활함". 반면, $L_{KL}$에 의해 이미 잘 정규화된 저차원 잠재 공간 $Z$는 $X$보다 훨씬 밀도 있고 구조적이므로, 이 공간에서 '진짜' 데이터의 분포를 학습하는 것이 더 효율적임.
* **$D_z$의 역할:** $Z$ 공간 내의 어떤 벡터 $z$가 '진짜 데이터로부터 유래한 것'($z_{\text{real}}$)인지, '단순 노이즈(사전 분포)'($z_{\text{fake}}$)인지 판별함.
* **$z_{\text{real}}$의 정의:** $D_z$의 과적합을 방지하고 VAE의 확률적 특성을 반영하기 위해, $z_{\text{real}}$은 인코더 출력 $\mu$의 고정된 집합이 아님.
    * $z_{\text{real}}$은 각 $x_i$에 대해 추론된 분포 $\mathcal{N}(\mu_i, \sigma_i^2)$에서 **리파라미터화(reparameterization)를 통해 실시간으로 샘플링된 $z_i$**임.
        * 이때 reparameterization이란, VAE에서 확률적 샘플링 과정을 미분 가능하게 만들기 위한 수학적 기법임. 여기서 인코더의 출력인 평균 $\mu_i$와 표준편차 $\sigma_i$에서 확률적인 샘플링을 통해 $z_i$를 생성하는데 이 랜덤 샘플링 과정이 수학적으로 블랙박스이기 때문에 최종 손실에서부터 역전파가 불가능함. 이를 해결하기 위해 $\epsilon$이라는 표준 정규분포에서 샘플링된 노이즈를 이용하여 $z_i$를 다음과 같이 결정론적으로 표현함.

    * $$z_i = \mu_i + \epsilon \cdot \sigma_i \quad (\text{단, } \epsilon \sim \mathcal{N}(0, I))$$
    * 위 수식은 인코더가 추정한 특정 잠재벡터의 분포에서 샘플링한 것과 수학적으로/통계적으로 동일한 효과를 가짐.
    * 이는 $D_z$가 $\mu_i$라는 '점'이 아닌 **'확률적 영역'**을 '진짜'로 학습하게 하며, 데이터의 **총 사후 분포 $q(z)$**의 실제 형태를 학습하도록 유도함.
* **$z_{\text{fake}}$의 정의:** $z_{\text{fake}}$는 $L_{KL}$의 목표 분포였던 **사전 분포 $p(z)$**에서 샘플링된 벡터임.
    * $$z_j \sim p(z) = \mathcal{N}(0, I)$$
* **$D_z$의 학습 결과:** $D_z$는 $p(z)$라는 '전체 공간' 내에서 $q(z)$라는 '실제 데이터가 존재하는 영역(매니폴드)'을 정확히 식별해내는 **밀도 추정기(density estimator)**가 됨. $D_z(z)$의 출력값은 $z$가 $q(z)$에 속할 확률, 즉 **"헷갈림의 밀도 지도"**가 됨.

**4. 탐색 알고리즘 (Score-based Guidance)**

$D_z$가 잠재 공간의 밀도를 학습했다면, 이 공간은 미분 가능한(differentiable) '점수 공간'이 됨.

* **탐색의 근거:** $D_z(z)$의 값은 에너지 기반 모델(EBM)의 $p(z) \propto \exp(-E(z))$에서의 에너지 $E(z)$와 유사한 역할을 함 ($D_z(z) \uparrow \implies E(z) \downarrow$).
* **그래디언트 활용:** $D_z(z)$의 **그래디언트(Gradient) $\nabla_z D_z(z)$**는 현재 $z$ 지점에서 $D_z$의 점수를 가장 가파르게 증가시키는 방향, 즉 **가장 가까운 $z_{\text{real}}$ 매니폴드(산등성이)로 향하는 방향**을 제시함.
* **안내된 탐색 (Guided Exploration):** 이 그래디언트를 활용하여, $z_t$ 지점에서 목표 지점 $z_B$로 이동하되 '진짜' 영역을 벗어나지 않도록 안내하는 탐색 알고리즘을 정의할 수 있음.
    $$z_{t+1} = z_t + \alpha \nabla_z \log D_z^*(z_t) + \gamma (z_B - z_t)$$
    * $\alpha \nabla_z \log D_z^*(z_t)$ : $z_t$를 $D_z^*$의 점수가 높은 (즉, '진짜'일 확률이 높은) 방향으로 이동시키는 **경사 상승(gradient ascent)** 항. (수치적 안정을 위해 $D_z^*$ 대신 $\log D_z^*$를 사용)
    * $\gamma (z_B - z_t)$ : $z_t$를 목표 $z_B$ 방향으로 이동시키는 **목표 지향(target-seeking)** 항. (이는 $||z_B - z_t||_2^2$에 대한 경사 하강 $-\nabla_z ||z_B - z_t||_2^2$와 방향이 같음)

이 접근법은 잠재 공간 내에서 $D_z$의 점수(밀도)를 나침반 삼아 '의미 있는' 영역을 벗어나지 않으면서 안정적으로 데이터를 탐색하고 조작(manipulation)하는 것을 가능하게 함.